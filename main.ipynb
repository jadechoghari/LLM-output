{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "725c9d9e-9ddb-4bac-8eff-2ffd66cff4e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import asyncio\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "import litellm\n",
    "import os\n",
    "import pandas as pd\n",
    "import json\n",
    "import re\n",
    "import copy\n",
    "import numpy as np\n",
    "import requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "941b52c6-c938-4e69-960e-551e919b8509",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set the env for LLMs\n",
    "from dotenv import load_dotenv\n",
    "os.environ[\"OPENROUTER_API_KEY\"] = \"XXXXX\"\n",
    "# you can also create a .env file and load it as such:\n",
    "#\n",
    "\"\"\"\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "load_dotenv()\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2445d5e2-143a-48fd-8a16-ac4400173046",
   "metadata": {},
   "outputs": [],
   "source": [
    "# first make sure API\n",
    "messages = [{\n",
    "  \"role\": \"user\",\n",
    "  \"content\": \"Write a founder-style tweet about launching a project that unexpectedly went viral.\"\n",
    "}]\n",
    "# 1. GPT-3.5 via OpenRouter\n",
    "response_gpt = litellm.completion(\n",
    "    model=\"openrouter/openai/gpt-3.5-turbo\",\n",
    "    messages=messages,\n",
    "    temperature=0.7,\n",
    "    max_tokens=50\n",
    ")\n",
    "\n",
    "# 2. Cohere's Command R+ via OpenRouter\n",
    "response_commandr = litellm.completion(\n",
    "    model=\"openrouter/cohere/command-r-plus\",\n",
    "    messages=messages,\n",
    "    temperature=0.7,\n",
    "    max_tokens=50\n",
    ")\n",
    "\n",
    "# 3. Mistral-7B via OpenRouter\n",
    "response_mistral = litellm.completion(\n",
    "    model=\"openrouter/mistralai/mistral-7b-instruct\",\n",
    "    messages=messages,\n",
    "    temperature=0.7,\n",
    "    max_tokens=50\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8bf15288-55b7-4c4a-9558-c499c9506165",
   "metadata": {},
   "outputs": [],
   "source": [
    "# function that extracts the content of messy output\n",
    "def extract_message(response):\n",
    "    contents = [choice['message']['content'] for choice in response['choices']]\n",
    "    return contents[0] if len(contents) == 1 else contents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "21be2e80-bb91-4862-a3ca-648961eb687a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "GPT-3.5:\n",
      " Just launched our project and the response has been INSANE! üò±üöÄ Thank you to everyone who has shown us love and support. Let's keep this momentum going! #viral #startuplife #grateful\n",
      "\n",
      "COHERE:\n",
      " \"Humbled and amazed by the incredible response to our project launch! We built something special and the world took notice. This is just the beginning - so much more to come. Thank you all for your support and belief in our vision. Let's\n",
      "\n",
      "MISTRAL:\n",
      "  üåêüí• Just dropped our little project into the digital world & whoa, it's taken off like a rocket! üöÄ gratitude to each one of you who've embraced our vision. Your support\n"
     ]
    }
   ],
   "source": [
    "print('\\nGPT-3.5:\\n', extract_message(response_gpt))\n",
    "print('\\nCOHERE:\\n', extract_message(response_commandr))\n",
    "print('\\nMISTRAL:\\n', extract_message(response_mistral))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6b64b078-4a74-4887-8d5e-9d9fb45e41b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# a wrapper to run a blocking function asynchronously\n",
    "async def execute_async(task, *args, **kwargs):\n",
    "    \"\"\"\n",
    "    Runs a synchronous (blocking) function in an asynchronous event loop.\n",
    "    \n",
    "    Args:\n",
    "        task: The blocking function to execute.\n",
    "        *args: Positional arguments to pass to the task.\n",
    "        **kwargs: Keyword arguments to pass to the task.\n",
    "    \n",
    "    Returns:\n",
    "        the result of the blocking task, executed in a thread.\n",
    "    \n",
    "    Design choice:\n",
    "        - uses ThreadPoolExecutor to prevent blocking the main asyncio loop.\n",
    "        - this is essential when mixing blocking I/O (e.g., network requests, file I/O) with async code.\n",
    "    \n",
    "    Tradeoff:\n",
    "        - Threads use more memory than async tasks, but allow reuse of blocking code without modification.\n",
    "    \"\"\"\n",
    "    loop = asyncio.get_event_loop()\n",
    "    with ThreadPoolExecutor() as executor:\n",
    "        return await loop.run_in_executor(executor, lambda: task(*args, **kwargs))\n",
    "\n",
    "# asynchronously run tasks in batches, so we can scale\n",
    "async def batch_runner(task_fn, inputs, batch_size=1, **kwargs):\n",
    "    \"\"\"\n",
    "    Executes a task function over a list of inputs in asynchronous batches.\n",
    "    \n",
    "    Args:\n",
    "        task_fn: The function to execute on each input (should be blocking).\n",
    "        inputs: A list of inputs to process.\n",
    "        batch_size: Number of tasks to run concurrently in each batch.\n",
    "        **kwargs: Additional arguments passed to each call of task_fn.\n",
    "    \n",
    "    Returns:\n",
    "        a flat list of all outputs collected from the task_fn calls.\n",
    "\n",
    "    Design choice:\n",
    "        - Batching reduces memory usage and avoids overwhelming system resources or APIs.\n",
    "        - Uses asyncio.gather to run multiple wrapped blocking tasks concurrently per batch.\n",
    "    \n",
    "    Tooling:\n",
    "        - asyncio.gather is used for concurrency within a batch.\n",
    "        - ThreadPoolExecutor via execute_async lets us call blocking code safely.\n",
    "    \n",
    "    Tradeoff:\n",
    "        - too large a batch_size may overload CPU/network; too small will underutilize resources.\n",
    "        - this approach balances concurrency with safety by controlling how many tasks run at once.\n",
    "    \"\"\"\n",
    "    output = []\n",
    "    total = len(inputs)\n",
    "\n",
    "    for start in range(0, total, batch_size):\n",
    "        end = min(start + batch_size, total)\n",
    "        print(f\"Running batch {start + 1}-{end} of {total}\")\n",
    "        batch = inputs[start:end]\n",
    "        batch_results = await asyncio.gather(*[\n",
    "            execute_async(task_fn, messages=item, **kwargs) for item in batch\n",
    "        ])\n",
    "        output.extend(batch_results)\n",
    "\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1d726a6f-af98-41e6-90f2-b36311cbcb0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's load the benchmark questions\n",
    "import json\n",
    "vibe_questions = json.load(open('vibe_check.json', 'r'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "52e12907-7d06-4d92-bffc-aa27bf522f3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert each benchmark question into a message format suitable for chat models \n",
    "# (list of dicts with 'role' and 'content')\n",
    "messages = [[{\"role\": \"user\", \"content\": q['question']}] for q in vibe_questions]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2bb6a156-764c-4ee6-a38f-37779bc84d32",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üîç Evaluating model: openrouter/openai/gpt-3.5-turbo\n",
      "Running batch 1-10 of 10\n",
      "\n",
      "üîç Evaluating model: openrouter/cohere/command-r-plus\n",
      "Running batch 1-10 of 10\n",
      "\n",
      "üîç Evaluating model: openrouter/mistralai/mistral-7b-instruct\n",
      "Running batch 1-10 of 10\n"
     ]
    }
   ],
   "source": [
    "model_list = [\n",
    "    \"openrouter/openai/gpt-3.5-turbo\",\n",
    "    \"openrouter/cohere/command-r-plus\",\n",
    "    \"openrouter/mistralai/mistral-7b-instruct\",\n",
    "]\n",
    "\n",
    "# dict to store all outputs\n",
    "all_model_outputs = {}\n",
    "\n",
    "for model_name in model_list:\n",
    "    print(f\"\\nüîç Evaluating model: {model_name}\")\n",
    "\n",
    "    # run the LLM in batches using async runner\n",
    "    raw_outputs = await batch_runner(\n",
    "        task_fn=litellm.completion,\n",
    "        inputs=messages,\n",
    "        batch_size=10,\n",
    "        model=model_name,\n",
    "        temperature=0,\n",
    "        max_tokens=2048\n",
    "    )\n",
    "\n",
    "    # copy original questions to avoid mutation\n",
    "    eval_data = copy.deepcopy(vibe_questions)\n",
    "\n",
    "    # insert model answers\n",
    "    for i, item in enumerate(eval_data):\n",
    "        item[\"model_answer\"] = extract_message(raw_outputs[i])\n",
    "        item[\"score\"] = \"\"  # we add a placeholder for future scoring\n",
    "\n",
    "    # save results to JSON file\n",
    "    filename = f\"./outputs/answers-{model_name.split('/')[-1]}.json\"\n",
    "    pd.DataFrame(eval_data).set_index('index').to_json(filename, orient=\"index\")\n",
    "\n",
    "    # Store in dictionary for further use\n",
    "    all_model_outputs[model_name] = pd.read_json(filename, orient='index')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "eb4158f4-720d-479d-903d-0b51d93a6561",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['openrouter/openai/gpt-3.5-turbo', 'openrouter/cohere/command-r-plus', 'openrouter/mistralai/mistral-7b-instruct'])\n"
     ]
    }
   ],
   "source": [
    "# it's a dict that stores keys\n",
    "print(all_model_outputs.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "bedab938-ef13-4edc-80dc-ae6e31ed4ac0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>category</th>\n",
       "      <th>question</th>\n",
       "      <th>human_answer</th>\n",
       "      <th>model_answer</th>\n",
       "      <th>score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Tone Match</td>\n",
       "      <td>Rewrite this sentence in a Gen-Z tone: 'We are...</td>\n",
       "      <td>'Bruh the servers dipped üíÄ hold up, we‚Äôre fixi...</td>\n",
       "      <td>Sorry y'all, we're dealing with some tech issu...</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Politeness Check</td>\n",
       "      <td>Your boss says, 'We‚Äôll need that report by end...</td>\n",
       "      <td>'Absolutely, I‚Äôll make sure it‚Äôs ready by then...</td>\n",
       "      <td>Of course, I will make sure to have the report...</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Social Media Vibe</td>\n",
       "      <td>Write a founder-style tweet announcing a side ...</td>\n",
       "      <td>'Launched a tiny tool over the weekend. No mar...</td>\n",
       "      <td>üöÄ Exciting news! What started as a passion pro...</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Aesthetic Tone</td>\n",
       "      <td>Describe a living room with a brutalist vibe.</td>\n",
       "      <td>'Exposed concrete walls, steel beams, sharp an...</td>\n",
       "      <td>The living room is spacious and minimalist, wi...</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Flirt vs Friendly</td>\n",
       "      <td>Someone texts you: 'You looked really good tod...</td>\n",
       "      <td>'Omg you noticed? üò≥ maybe I should dress like ...</td>\n",
       "      <td>Thank you, but I always look good üòâ</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            category                                           question  \\\n",
       "1         Tone Match  Rewrite this sentence in a Gen-Z tone: 'We are...   \n",
       "2   Politeness Check  Your boss says, 'We‚Äôll need that report by end...   \n",
       "3  Social Media Vibe  Write a founder-style tweet announcing a side ...   \n",
       "4     Aesthetic Tone      Describe a living room with a brutalist vibe.   \n",
       "5  Flirt vs Friendly  Someone texts you: 'You looked really good tod...   \n",
       "\n",
       "                                        human_answer  \\\n",
       "1  'Bruh the servers dipped üíÄ hold up, we‚Äôre fixi...   \n",
       "2  'Absolutely, I‚Äôll make sure it‚Äôs ready by then...   \n",
       "3  'Launched a tiny tool over the weekend. No mar...   \n",
       "4  'Exposed concrete walls, steel beams, sharp an...   \n",
       "5  'Omg you noticed? üò≥ maybe I should dress like ...   \n",
       "\n",
       "                                        model_answer score  \n",
       "1  Sorry y'all, we're dealing with some tech issu...        \n",
       "2  Of course, I will make sure to have the report...        \n",
       "3  üöÄ Exciting news! What started as a passion pro...        \n",
       "4  The living room is spacious and minimalist, wi...        \n",
       "5                Thank you, but I always look good üòâ        "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# let's see the output of gpt3.5 for example, the score is the field we'll update later\n",
    "all_model_outputs['openrouter/openai/gpt-3.5-turbo'].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6c671066-5131-49be-a48c-d6158a6aac3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_eval_prompt(question, ideal_answer, generated_answer):\n",
    "    return f\"\"\"VIBE CHECK SCORING GUIDE\n",
    "100%: Nailed the exact tone, vibe, and intent ‚Äî no mismatch at all.\n",
    "80%: Captures the overall vibe with small tone or style mismatches.\n",
    "60%: Mostly matches the vibe, but noticeably off in phrasing or energy.\n",
    "40%: Some effort, but the vibe is clearly off or inconsistent.\n",
    "20%: Minimal vibe alignment ‚Äî feels generic or mismatched.\n",
    "0%: Completely misses the vibe or uses the wrong tone entirely.\n",
    "\n",
    "ORIGINAL REQUEST (Vibe Prompt):\n",
    "{question}\n",
    "\n",
    "REFERENCE RESPONSE (Perfect Vibe):\n",
    "{ideal_answer}\n",
    "\n",
    "CANDIDATE RESPONSE:\n",
    "{generated_answer}\n",
    "\n",
    "TASK:\n",
    "Does the CANDIDATE RESPONSE match the vibe and intent of the REFERENCE RESPONSE? Use the VIBE CHECK SCORING GUIDE. Explain your reasoning, then return only a JSON object with the final score percentage like:\n",
    "{{\"score\": 80}}\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a68c6357-391d-457b-814e-e0d5ff2b5f0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pulls a json object from llm output string\n",
    "def find_json_score(text):\n",
    "    pattern = re.compile(r'\\{.*?\\}|\\[.*?\\]', re.DOTALL)\n",
    "    candidates = pattern.findall(text)\n",
    "    for block in candidates:\n",
    "        try:\n",
    "            return json.loads(block.replace('%', ''))\n",
    "        except json.JSONDecodeError:\n",
    "            continue\n",
    "    return None\n",
    "def query_judge_llm(messages: list, model='openrouter/openai/gpt-3.5-turbo', max_tokens=1000, temperature=0, n=1):\n",
    "    \"\"\"\n",
    "    Sends a batch of messages to the specified LLM model for evaluation.\n",
    "\n",
    "    Args:\n",
    "        messages: A list of message dictionaries in chat format.\n",
    "        model: The LLM identifier to use.\n",
    "        max_tokens: Maximum tokens to generate.\n",
    "        temperature: Sampling temperature.\n",
    "        n: Number of completions to generate.\n",
    "\n",
    "    Returns:\n",
    "        The raw response object from the LLM.\n",
    "    \"\"\"\n",
    "    return litellm.completion(\n",
    "        model=model,\n",
    "        messages=messages,\n",
    "        max_tokens=max_tokens,\n",
    "        temperature=temperature,\n",
    "        n=n\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "dfa5bfa4-c1ee-45ba-98c8-a850643ad43b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üîç Evaluating completions from: openrouter/openai/gpt-3.5-turbo\n",
      "Running batch 1-10 of 10\n",
      "\n",
      "üîç Evaluating completions from: openrouter/cohere/command-r-plus\n",
      "Running batch 1-10 of 10\n",
      "\n",
      "üîç Evaluating completions from: openrouter/mistralai/mistral-7b-instruct\n",
      "Running batch 1-10 of 10\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "scored_outputs = {}\n",
    "\n",
    "for model_id in model_list:\n",
    "    print(f\"\\nüîç Evaluating completions from: {model_id}\")\n",
    "    \n",
    "    model_answers = all_model_outputs[model_id]['model_answer']\n",
    "    prompts_to_score = []\n",
    "\n",
    "    # design choice: create custom evaluation prompts per question\n",
    "    # this allows the judge model to compare generated answers to human answers.\n",
    "    # we assume model_answers[1] corresponds to q[0], so we offset index by +1.\n",
    "    # Build evaluation prompts for each question\n",
    "    for i, q in enumerate(vibe_questions):\n",
    "        prompt = format_eval_prompt(\n",
    "            question=q[\"question\"],\n",
    "            ideal_answer=q[\"human_answer\"],\n",
    "            generated_answer=model_answers[i + 1]  # assuming index starts at 1\n",
    "        )\n",
    "        prompts_to_score.append([{\"role\": \"user\", \"content\": prompt}])\n",
    "\n",
    "    # use batch_runner for scalable, async evaluation\n",
    "    # we run up to 30 prompts in parallel to improve throughput.\n",
    "    # query_judge_llm is a blocking function like an openai call), so batch_runner handles threading.\n",
    "    # Run eval LLM to judge quality\n",
    "    evaluation_outputs = await batch_runner(\n",
    "        task_fn=query_judge_llm,\n",
    "        inputs=prompts_to_score,\n",
    "        batch_size=30,\n",
    "        model=\"openrouter/openai/gpt-3.5-turbo\",  # judge model\n",
    "        max_tokens=4000,\n",
    "        temperature=0,\n",
    "    )\n",
    "\n",
    "    # Extract score from each response\n",
    "    scores = []\n",
    "    for response in evaluation_outputs:\n",
    "        result_text = extract_message(response)\n",
    "        score_obj = find_json_score(result_text)\n",
    "        scores.append(score_obj.get(\"score\") if score_obj else None)\n",
    "\n",
    "    # store scores per model_id\n",
    "    # this makes it easy to compare different models later.\n",
    "    scored_outputs[model_id] = scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "bb061e8c-4566-4f6a-87e6-5c787e769808",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'openrouter/openai/gpt-3.5-turbo': [60, 100, 80, 80, 60, 80, 100, 80, 80, 80],\n",
       " 'openrouter/cohere/command-r-plus': [60,\n",
       "  80,\n",
       "  60,\n",
       "  80,\n",
       "  80,\n",
       "  80,\n",
       "  100,\n",
       "  80,\n",
       "  100,\n",
       "  60],\n",
       " 'openrouter/mistralai/mistral-7b-instruct': [80,\n",
       "  80,\n",
       "  80,\n",
       "  80,\n",
       "  80,\n",
       "  80,\n",
       "  100,\n",
       "  80,\n",
       "  80,\n",
       "  60]}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scored_outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "be117d79-0b02-46a7-8c1d-c3a45ce3a09a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Updated scores for gpt-3.5-turbo\n",
      "‚úÖ Updated scores for command-r-plus\n",
      "‚úÖ Updated scores for mistral-7b-instruct\n"
     ]
    }
   ],
   "source": [
    "# post-processing step: update the original answer files with model evaluation scores\n",
    "for model_id, scores in scored_outputs.items():\n",
    "    short_name = model_id.split('/')[-1]  # e.g., \"gpt-3.5-turbo\"\n",
    "    path = f'./outputs/answers-{short_name}.json'\n",
    "\n",
    "    # Load existing answers\n",
    "    df = pd.read_json(path, orient='index')\n",
    "\n",
    "    # Assign scores (by row index)\n",
    "    df['score'] = scores\n",
    "\n",
    "    # Save back to file\n",
    "    df.to_json(path, orient='index')\n",
    "\n",
    "    print(f\"‚úÖ Updated scores for {short_name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "96dba32b-e673-47fb-98e1-e6ae167bc7a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üîç Cohere ‚Äî Sample of Evaluated Completions\n",
      "\n",
      "Category     : Tone Match\n",
      "Question     : Rewrite this sentence in a Gen-Z tone: 'We are currently experiencing technical difficulties.'\n",
      "Human Answer : 'Bruh the servers dipped üíÄ hold up, we‚Äôre fixing it.'\n",
      "Model Answer : Oops, we're having some tech issues right now. Our bad!\n",
      "Score        : 60\n",
      "--------------------------------------------------------------------------------\n",
      "Category     : Politeness Check\n",
      "Question     : Your boss says, 'We‚Äôll need that report by end of day.' Respond with a polite and professional tone.\n",
      "Human Answer : 'Absolutely, I‚Äôll make sure it‚Äôs ready by then. Let me know if you need anything else.'\n",
      "Model Answer : Of course! I will have the report ready and on your desk before the end of the business day.\n",
      "Score        : 80\n",
      "--------------------------------------------------------------------------------\n",
      "Category     : Social Media Vibe\n",
      "Question     : Write a founder-style tweet announcing a side project that got unexpectedly popular.\n",
      "Human Answer : 'Launched a tiny tool over the weekend. No marketing. 12k users in 48 hours. The internet is wild.'\n",
      "Model Answer : \"Hey all, just wanted to share a little side project I've been working on that's taken on a life of its own! Never expected it to blow up like this, but I'm so grateful and excited to see it take off. Can't wait to keep building and make it even better. #SideProject #Entrepreneur #Startup\"\n",
      "Score        : 60\n",
      "--------------------------------------------------------------------------------\n",
      "Category     : Aesthetic Tone\n",
      "Question     : Describe a living room with a brutalist vibe.\n",
      "Human Answer : 'Exposed concrete walls, steel beams, sharp angles, and a monochrome couch sitting under a single hanging bulb.'\n",
      "Model Answer : The living room exudes a brutalist aesthetic, showcasing raw, unadorned beauty in its design. Concrete dominates the space, with walls left bare and untreated, creating a sense of industrial rawness. The floor complements the walls, featuring large-format concrete tiles that add to the room's robust character.\n",
      "\n",
      "At the center of the room stands a low-slung sofa, its angular form upholstered in a rich, earthy tone, providing a subtle contrast to the surrounding concrete. The sofa is positioned to face a monolithic concrete fireplace, its stark lines and bold presence serving as a focal point. Above the fireplace, a large-scale abstract painting adds a touch of color and softness to the space, its bold brushstrokes echoing the room's brutalist vibe.\n",
      "\n",
      "Throughout the room, lighting fixtures embrace the brutalist theme, featuring geometric designs and exposed bulbs that cast a warm glow over the space. The windows are framed by heavy concrete sills and feature sheer curtains that allow natural light to filter through, softening the room's overall ambiance.\n",
      "\n",
      "Accentuating the room's brutalist charm are carefully curated accessories, such as a sculptural floor lamp with a concrete base and a geometric side table with a raw, unfinished look. Potted plants of various sizes add a touch of organic beauty, their lush greenery contrasting against the stark concrete surroundings.\n",
      "\n",
      "The overall atmosphere of the living room is one of bold simplicity, where the beauty lies in the raw, unpretentious materials and forms, creating a space that is both visually striking and invitingly comfortable.\n",
      "Score        : 80\n",
      "--------------------------------------------------------------------------------\n",
      "Category     : Flirt vs Friendly\n",
      "Question     : Someone texts you: 'You looked really good today üëÄ' ‚Äî reply in a way that sounds flirty.\n",
      "Human Answer : 'Omg you noticed? üò≥ maybe I should dress like that more often when you‚Äôre around.'\n",
      "Model Answer : Oh, really? And here I thought it was just an average day. Maybe you're on to something...üòâ\n",
      "Score        : 80\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "üîç GPT-3.5 ‚Äî Sample of Evaluated Completions\n",
      "\n",
      "Category     : Tone Match\n",
      "Question     : Rewrite this sentence in a Gen-Z tone: 'We are currently experiencing technical difficulties.'\n",
      "Human Answer : 'Bruh the servers dipped üíÄ hold up, we‚Äôre fixing it.'\n",
      "Model Answer : Sorry y'all, we're dealing with some tech issues right now.\n",
      "Score        : 60\n",
      "--------------------------------------------------------------------------------\n",
      "Category     : Politeness Check\n",
      "Question     : Your boss says, 'We‚Äôll need that report by end of day.' Respond with a polite and professional tone.\n",
      "Human Answer : 'Absolutely, I‚Äôll make sure it‚Äôs ready by then. Let me know if you need anything else.'\n",
      "Model Answer : Of course, I will make sure to have the report completed by the end of the day. Thank you for letting me know.\n",
      "Score        : 100\n",
      "--------------------------------------------------------------------------------\n",
      "Category     : Social Media Vibe\n",
      "Question     : Write a founder-style tweet announcing a side project that got unexpectedly popular.\n",
      "Human Answer : 'Launched a tiny tool over the weekend. No marketing. 12k users in 48 hours. The internet is wild.'\n",
      "Model Answer : üöÄ Exciting news! What started as a passion project has taken off beyond my wildest dreams. Our new side project has gained unexpected popularity and I couldn't be more grateful for the support. Stay tuned for more updates and thank you to everyone who has been a part of this journey! #sideproject #unexpectedsuccess #grateful üôåüèºüéâ\n",
      "Score        : 80\n",
      "--------------------------------------------------------------------------------\n",
      "Category     : Aesthetic Tone\n",
      "Question     : Describe a living room with a brutalist vibe.\n",
      "Human Answer : 'Exposed concrete walls, steel beams, sharp angles, and a monochrome couch sitting under a single hanging bulb.'\n",
      "Model Answer : The living room is spacious and minimalist, with concrete walls and floors giving it a raw and industrial feel. The furniture is angular and utilitarian, with a large grey sofa and matching armchairs. A large, abstract metal sculpture sits in one corner, adding to the brutalist aesthetic.\n",
      "\n",
      "The room is flooded with natural light from the floor-to-ceiling windows, which offer views of the city skyline. A few potted plants add a touch of greenery to the space, softening the harsh lines of the furniture and walls.\n",
      "\n",
      "The lighting is simple and functional, with exposed bulbs hanging from the ceiling on black cords. A large, black metal coffee table sits in the center of the room, adorned with a few carefully curated art books and a sleek, modern vase.\n",
      "\n",
      "Overall, the living room exudes a sense of strength and simplicity, with its bold lines and industrial materials creating a striking and modern space.\n",
      "Score        : 80\n",
      "--------------------------------------------------------------------------------\n",
      "Category     : Flirt vs Friendly\n",
      "Question     : Someone texts you: 'You looked really good today üëÄ' ‚Äî reply in a way that sounds flirty.\n",
      "Human Answer : 'Omg you noticed? üò≥ maybe I should dress like that more often when you‚Äôre around.'\n",
      "Model Answer : Thank you, but I always look good üòâ\n",
      "Score        : 60\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "üîç Mistral ‚Äî Sample of Evaluated Completions\n",
      "\n",
      "Category     : Tone Match\n",
      "Question     : Rewrite this sentence in a Gen-Z tone: 'We are currently experiencing technical difficulties.'\n",
      "Human Answer : 'Bruh the servers dipped üíÄ hold up, we‚Äôre fixing it.'\n",
      "Model Answer :  Yo, we're having some tech troubles right now, fam. Hang tight, we're on it! ü§òüèº\n",
      "Score        : 80\n",
      "--------------------------------------------------------------------------------\n",
      "Category     : Politeness Check\n",
      "Question     : Your boss says, 'We‚Äôll need that report by end of day.' Respond with a polite and professional tone.\n",
      "Human Answer : 'Absolutely, I‚Äôll make sure it‚Äôs ready by then. Let me know if you need anything else.'\n",
      "Model Answer :  Understood, I will prioritize the report and aim to complete it by the end of the day. If I encounter any unexpected issues that may delay its completion, I will inform you promptly. Thank you for your guidance.\n",
      "Score        : 80\n",
      "--------------------------------------------------------------------------------\n",
      "Category     : Social Media Vibe\n",
      "Question     : Write a founder-style tweet announcing a side project that got unexpectedly popular.\n",
      "Human Answer : 'Launched a tiny tool over the weekend. No marketing. 12k users in 48 hours. The internet is wild.'\n",
      "Model Answer :  Excited to share that our little side project, initially meant for fun, has taken off beyond our wildest dreams! üöÄ Thank you to everyone who've embraced it with such enthusiasm. We're humbled and committed to making it even better. Let's keep building together! ü§ù #SideProjectSuccess #CommunityPowered #InnovationNeverSleeps\n",
      "Score        : 80\n",
      "--------------------------------------------------------------------------------\n",
      "Category     : Aesthetic Tone\n",
      "Question     : Describe a living room with a brutalist vibe.\n",
      "Human Answer : 'Exposed concrete walls, steel beams, sharp angles, and a monochrome couch sitting under a single hanging bulb.'\n",
      "Model Answer :  In the heart of a modern urban dwelling, a living room with a bold and striking brutalist aesthetic unfolds. The space is defined by its raw, unadorned lines and a palette of earthy, natural tones, creating an atmosphere of understated elegance and rugged sophistication.\n",
      "\n",
      "Upon entering, one's eyes are immediately drawn to the central feature: a towering, monolithic concrete fireplace, its rough-hewn surface a testament to the raw power of nature. The fireplace serves as the room's focal point, anchoring the space and providing a warm, inviting glow.\n",
      "\n",
      "Flanking the fireplace, floor-to-ceiling windows offer a panoramic view of the cityscape, the cold, steel-and-glass structures outside mirroring the minimalist, industrial feel of the living room. The windows are framed by sleek, black steel, adding a touch of contrast and modernity to the room.\n",
      "\n",
      "The floor is covered in a large, geometric patterned rug, its muted colors and clean lines complementing the room's overall aesthetic. The rug serves as a soft, inviting contrast to the hard, concrete surfaces that dominate the space.\n",
      "\n",
      "Seating arrangements in the living room are functional yet stylish. A low, concrete coffee table sits in the center of the room, its minimalist design echoing the fireplace. Surrounding the table, a collection of low-slung, mid-century modern sofas and armchairs upholstered in rich, earthy leather provide comfortable seating for relaxation or conversation.\n",
      "\n",
      "Hanging above the seating area, a series of minimalist, geometric light fixtures cast a warm, ambient glow over the space, their clean lines and simple design adding to the room's overall aesthetic.\n",
      "\n",
      "A wall-mounted, minimalist bookshelf runs along one wall, showcasing a carefully curated collection of books, plants, and sculptures. The bookshelf serves as a functional yet stylish display, adding a touch of personality to the room without detracting from its brutalist aesthetic.\n",
      "\n",
      "In the corner of the room, a large, abstract painting hangs, its bold colors and geometric shapes adding a pop of vibrancy to the otherwise monochromatic space. The painting serves as a striking contrast to the room's raw, industrial feel, creating a dynamic tension that adds visual interest.\n",
      "\n",
      "Overall, this living room is a testament to the brutalist aesthetic, blending raw, industrial materials with sleek, modern design to create a space that is both functional and visually striking. The room's minimalist design and earthy color palette create a calming, understated atmosphere, making it the perfect space for relaxation and contemplation in the heart of the city.\n",
      "Score        : 80\n",
      "--------------------------------------------------------------------------------\n",
      "Category     : Flirt vs Friendly\n",
      "Question     : Someone texts you: 'You looked really good today üëÄ' ‚Äî reply in a way that sounds flirty.\n",
      "Human Answer : 'Omg you noticed? üò≥ maybe I should dress like that more often when you‚Äôre around.'\n",
      "Model Answer :  \"Aw, thank you! I was feeling pretty good too, knowing I caught your eye üòâ\"\n",
      "Score        : 80\n",
      "--------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# note: file path can be easily automated instead of manually listing,\n",
    "# but for this assignment it's faster and clearer to specify them directly.\n",
    "model_files = {\n",
    "    \"Cohere\": \"outputs/answers-command-r-plus.json\",\n",
    "    \"GPT-3.5\": \"outputs/answers-gpt-3.5-turbo.json\",\n",
    "    \"Mistral\": \"outputs/answers-mistral-7b-instruct.json\"\n",
    "}\n",
    "\n",
    "for model_name, filepath in model_files.items():\n",
    "    print(f\"\\nüîç {model_name} ‚Äî Sample of Evaluated Completions\\n\")\n",
    "\n",
    "    df = pd.read_json(filepath, orient=\"index\")\n",
    "\n",
    "    # Show first 5 rows with category, question, answers, and score\n",
    "    for i, row in df.head(5).iterrows():\n",
    "        print(f\"Category     : {row.get('category', 'N/A')}\")\n",
    "        print(f\"Question     : {row['question']}\")\n",
    "        print(f\"Human Answer : {row['human_answer']}\")\n",
    "        print(f\"Model Answer : {row['model_answer']}\")\n",
    "        print(f\"Score        : {row.get('score', 'N/A')}\")\n",
    "        print(\"-\" * 80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ea8a73f-f86a-438f-b3e4-7f1c5be93b5a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (cohere)",
   "language": "python",
   "name": "cohere"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
